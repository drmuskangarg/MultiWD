{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b0qDOxcIRQz"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQL5XoaoIUR5"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-X5eH2zE3vb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import f1_score\n",
        "from torch import nn\n",
        "# Replace 'train_data.csv' and 'test_data.csv' with the actual paths to your train and test CSV files\n",
        "train_data_path = 'path for training data'\n",
        "test_data_path = 'path for testing data'\n",
        "\n",
        "# Replace 'text' with the actual name of the text column in your dataframe\n",
        "text_column = 'text'\n",
        "\n",
        "# Replace 'label_1' to 'label_6' with the actual names of the label columns in your dataframe\n",
        "label_columns = ['Spiritual', 'Physical', 'Intellectual', 'Social', 'Vocational', 'Emotional']\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_data(data):\n",
        "    inputs = tokenizer(data['text'].tolist(), padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "# PyTorch Dataset\n",
        "class MentalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = (pred.predictions > 0).astype(int)  # Apply a threshold to obtain binary predictions\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, matthews_corrcoef\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, matthews_corrcoef\n",
        "\n",
        "\n",
        "#compute metrics2 geneate metrics for each label.\n",
        "def compute_metrics2(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = (pred.predictions > 0).astype(int)  # Apply a threshold to obtain binary predictions\n",
        "    \n",
        "    acc = accuracy_score(labels, preds)\n",
        "    overall_mcc = matthews_corrcoef(labels.ravel(), preds.ravel())  # Calculate overall MCC\n",
        "    \n",
        "    # Calculate per-label metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None)\n",
        "    mcc = [matthews_corrcoef(labels[:, i], preds[:, i]) for i in range(labels.shape[1])]\n",
        "    label_accuracy = [accuracy_score(labels[:, i], preds[:, i]) for i in range(labels.shape[1])]\n",
        "    \n",
        "    metrics = {\"accuracy\": acc, \"overall_mcc\": overall_mcc}\n",
        "    \n",
        "    # Add per-label metrics to the metrics dictionary\n",
        "    for i, label in enumerate(label_columns):\n",
        "        metrics[f\"{label}_accuracy\"] = label_accuracy[i]\n",
        "        metrics[f\"{label}_precision\"] = precision[i]\n",
        "        metrics[f\"{label}_recall\"] = recall[i]\n",
        "        metrics[f\"{label}_f1\"] = f1[i]\n",
        "        metrics[f\"{label}_mcc\"] = mcc[i]\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir='proj')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir='proj', num_labels=len(label_columns))\n",
        "\n",
        "# Change the model's classification head for multi-label classification\n",
        "model.classifier = nn.Linear(model.classifier.in_features, len(label_columns))\n",
        "model.config.id2label = {i: label for i, label in enumerate(label_columns)}\n",
        "model.config.label2id = {label: i for i, label in enumerate(label_columns)}\n",
        "\n",
        "# Change the loss function to BCEWithLogitsLoss for multi-label classification\n",
        "model.loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        " \n",
        "\n",
        "# Tokenize data\n",
        "tokenized_train_data = tokenize_data(train_data)\n",
        "tokenized_test_data = tokenize_data(test_data)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MentalDataset(tokenized_train_data, train_data[label_columns].values)\n",
        "test_dataset = MentalDataset(tokenized_test_data, test_data[label_columns].values)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "# Evaluate on test dataset\n",
        "test_results = trainer.predict(test_dataset)\n",
        "test_metrics = compute_metrics(test_results)\n",
        "print(\"Test set results:\", test_metrics)\n",
        "test_metrics2 = compute_metrics2(test_results)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calculate and print classification report separately\n",
        "labels = test_results.label_ids\n",
        "preds = (test_results.predictions > 0).astype(int)\n",
        "class_report = classification_report(labels, preds, target_names=label_columns)\n",
        "print(\"Classification report:\\n\", class_report)\n",
        "\n",
        "# Save predictions to CSV file\n",
        "predictions_df = pd.DataFrame()\n",
        "predictions_df[label_columns] = test_results.predictions\n",
        "predictions_df[\"text\"] = test_data[\"text\"]\n",
        "predictions_df.to_csv(\"path/to/save/predictions\", index=False)\n",
        "\n",
        "#2nd metrics\n",
        "print(\"Test set results:\", test_metrics2)\n",
        "print(\"Accuracy: {:.4f}\".format(test_metrics2[\"accuracy\"]))\n",
        "print(\"Overall MCC: {:.4f}\".format(test_metrics2[\"overall_mcc\"]))\n",
        "\n",
        "for label in label_columns:\n",
        "    print(f\"\\n{label}:\")\n",
        "    print(f\"  Accuracy: {test_metrics2[f'{label}_accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {test_metrics2[f'{label}_precision']:.4f}\")\n",
        "    print(f\"  Recall: {test_metrics2[f'{label}_recall']:.4f}\")\n",
        "    print(f\"  F1 Score: {test_metrics2[f'{label}_f1']:.4f}\")\n",
        "    print(f\"  MCC: {test_metrics2[f'{label}_mcc']:.4f}\")\n",
        "\n",
        "\n"
      ]
    }
  ]
}